from langchain.document_loaders import SitemapLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores.faiss import FAISS
from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings
from langchain.storage import LocalFileStore
from langchain.schema.runnable import RunnablePassthrough, RunnableLambda
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.memory import ConversationSummaryBufferMemory
from langchain.callbacks.base import BaseCallbackHandler
from bs4 import BeautifulSoup
import json
import streamlit as st


class ChatCallbackHandler(BaseCallbackHandler):
    def __init__(self, *args, **kwargs):
        self.message = ""

    def on_llm_start(self, *args, **kwargs):
        self.message_box = st.empty()

    def on_llm_end(self, *args, **kwargs):
        save_message(self.message, "ai")

    def on_llm_new_token(self, token, *args, **kwargs):
        self.message += token
        self.message_box.markdown(self.message)


function = {
    "name": "answer_question",
    "description": "ì´ functionì€ íš¨ìœ¨ì ìœ¼ë¡œ ì‚°ì¶œëœ ë‹µë³€ì„ ë°›ëŠ” functionì…ë‹ˆë‹¤.",
    "parameters": {
        "type": "object",
        "properties": {
            "isNew": {
                "type": "boolean",
                "description": "ê²°ê³¼ë¥¼ ìƒˆë¡œ ë§Œë“¤ì§€ ì•ˆ ë§Œë“¤ì§€ ì •í•˜ëŠ” propertyì„. (ê³¼ê±° ê¸°ë¡ì— ì´ë¯¸ ë‹µë³€ëœ ì •ë³´ê°€ ìˆë‹¤ë©´ ìƒˆë¡œ ë§Œë“¤ì§€ ì•Šìœ¼ë‹ˆ Falseì…ë‹ˆë‹¤.)",
            },
            "answer": {
                "type": "string",
                "description": "ê³¼ê±° ëŒ€í™” ê¸°ë¡ì— ë‹µë³€ëœ ì •ë³´ê°€ ìˆì„ ë•Œ ê·¸ ì •ë³´ë¥¼ í¬í•¨í•˜ëŠ” propertyì…ë‹ˆë‹¤. (ë§Œì•½ ë‹µë³€ëœ ê¸°ë¡ì´ ì—†ë‹¤ë©´ ì´ propertyëŠ” ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.)",
            },
        },
    },
    "required": ["isNew"],
}

llm = ChatOpenAI(
    temperature=0.1,
)

streaming_llm = ChatOpenAI(
    temperature=0.1,
    streaming=True,
    callbacks=[ChatCallbackHandler()],
)

function_llm = ChatOpenAI(
    temperature=0.1,
).bind(
    functions=[
        function,
    ],
    function_call={"name": "answer_question"},
)

if "memory" not in st.session_state:
    st.session_state["memory"] = ConversationSummaryBufferMemory(
        llm=llm, max_token_limit=1000
    )

answers_prompt = ChatPromptTemplate.from_template(
    """
ë‹¹ì‹ ì€ ë¬¸ì„œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ ë¬¸ì„œë“¤ì„ ë³´ê³  ì§ˆë¬¸ê³¼ ê´€ë ¨ ìˆëŠ” ë¶€ë¶„ì„ ì°¾ì•„ ë‹µí•˜ê³  ë‹µë³€ì— ëŒ€í•œ ì ìˆ˜ë¥¼ ë¶€ì—¬í•´ì•¼ í•©ë‹ˆë‹¤.
ì ìˆ˜ëŠ” 0ì ë¶€í„° 5ì ê¹Œì§€ ìˆìœ¼ë©° 0ì ì€ ì“¸ëª¨ ì—†ëŠ” ë‹µë³€, 5ì ì€ ë§¤ìš° ìœ ìš©í•œ ë‹µë³€ì…ë‹ˆë‹¤.

ì˜ˆì‹œ:

ì§ˆë¬¸: ë‹¬ì€ ì–¼ë§ˆë‚˜ ë©€ë¦¬ ìˆë‚˜ìš”?
ë‹µ: ë‹¬ì€ 384,400 km ë§Œí¼ ë–¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.
ì ìˆ˜: 5

ì§ˆë¬¸: ë‹¬ì€ ì–¼ë§ˆë‚˜ ë©€ë¦¬ ìˆë‚˜ìš”?
ë‹µ: ë‹¬ì€ 384,400 ëª…ì…ë‹ˆë‹¤.
ì ìˆ˜: 0

ì§ˆë¬¸: íƒœì–‘ì€ ì–¼ë§ˆë‚˜ ë©€ë¦¬ ìˆë‚˜ìš”?
ë‹µ: ì˜ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤.
ìŠ¤ì½”ì–´: 0

ì‚¬ìš©ìê°€ ì´ì „ ëŒ€í™” ê¸°ë¡ì— ê´€í•´ì„œ ì§ˆë¬¸ì„ í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
ê·¸ëŸ° ê²½ìš°ì—ëŠ” ì´ì „ ëŒ€í™” ê¸°ë¡ì„ ì°¸ê³ í•´ì„œ ë‹µë³€ì„ ë§Œë“¤ê³  ì ìˆ˜ë¥¼ ë¶€ì—¬í•´ ì£¼ì„¸ìš”.

ì´ì „ ëŒ€í™” ê¸°ë¡:
{memory}

ë‹¹ì‹ ì˜ ì°¨ë¡€ì…ë‹ˆë‹¤!

ë¬¸ì„œ: {context}

ì§ˆë¬¸: {question}
""",
)

choose_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            """
    ì£¼ì–´ì§„ ë‹µë³€ë“¤ë§Œ ì´ìš©í•´ì„œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.
     
    ì ìˆ˜ê°€ ë†’ì€ ë‹µë³€ë“¤ì„ ë” ì´ìš©í•˜ì„¸ìš”. (ë”ìš± ìœ ìš©í•œ ì •ë³´ì…ë‹ˆë‹¤)
    ê·¸ë¦¬ê³  ìµœê·¼ ì •ë³´ë“¤ì„ ìš°ì„ ì ìœ¼ë¡œ ì´ìš©í•˜ì„¸ìš”.
    ì ìˆ˜ê°€ ë‚®ì€ ë‹µë³€ì€ ê²°ê³¼ì— í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
    ê²°ê³¼ë¥¼ ìƒì„±í•˜ëŠ”ë° ì‚¬ìš©ëœ ë‹µë³€ì˜ ì¶œì²˜ë“¤ì„ ë³€ê²½í•˜ì§€ ë§ê³ , ê·¸ëŒ€ë¡œ ë°˜í™˜í•˜ì„¸ìš”.
    ì¶œì²˜ëŠ” ë¬´ì¡°ê±´ í‘œì‹œí•´ì•¼ í•˜ë©° ë‚ ì§œëŠ” í‘œì‹œí•˜ì§€ ì•Šì•„ë„ ë©ë‹ˆë‹¤.

    ë‹µë³€ë“¤: {answers}

    ì˜ˆì‹œ:(
    ì§ˆë¬¸: Next.js ê°•ì˜ëŠ” ì–¼ë§ˆì´ë©° ìˆ˜ê°•ìƒì€ ëª‡ëª…ì¸ê°€ìš”?
    ê²°ê³¼: Next.jsëŠ” 160,000ì›ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ í˜„ì¬ëŠ” í• ì¸ ì¤‘ì´ë¼ì„œ 75,000ì›ì…ë‹ˆë‹¤. ìˆ˜ê°•ìƒì€ 3866ëª…ì…ë‹ˆë‹¤. (ì¶œì²˜: https://discord.com/about)
    )
""",
        ),
        ("human", "{question}"),
    ]
)

cache_prompt = ChatPromptTemplate.from_template(
    """
ë‹¹ì‹ ì€ ê³¼ê±° ê¸°ë¡ì„ ë³´ê³  ìƒˆë¡œìš´ ì§ˆë¬¸ê³¼ ë¹„ìŠ·í•œ ì˜ë¯¸ë¥¼ ì§€ë‹Œ ê¸°ë¡ì„ ì°¾ì•„ì„œ ì´ë¯¸ aiê°€ ë‹µí•œ ë°ì´í„°ë¥¼ ë°˜í™˜í•´ì„œ íš¨ìœ¨ì ìœ¼ë¡œ ì¼í•˜ê²Œ í•´ì£¼ëŠ” ë¹„ì„œì…ë‹ˆë‹¤.
ë”°ë¼ì„œ ë‹¹ì‹ ì€ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ë‹¹ì‹ ì´ ìƒì„±í•´ì„œëŠ” ì•ˆë˜ë©° ê¸°ë¡ì—ì„œ ì°¾ì•„ì„œ ë°˜í™˜í•´ì•¼ í•©ë‹ˆë‹¤. ë§Œì•½ì— ì¡´ì¬í•˜ì§€ ì•Šë‹¤ë©´ ë°˜í™˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    
ì£¼ì–´ì§„ ê³¼ê±° ëŒ€í™” ê¸°ë¡ì—ì„œ ìƒˆë¡œìš´ ì§ˆë¬¸ê³¼ ë¹„ìŠ·í•œ ì˜ë¯¸ì˜ ì§ˆë¬¸ì„ í–ˆë˜ ê¸°ë¡ì„ ì°¾ê³  ê·¸ ì§ˆë¬¸ì´ aiì—ê²Œ ë‹µë³€ì´ ì™„ë£ŒëëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.
ë§Œì•½ aiì—ê²Œ ë‹µë³€ì´ ì™„ë£Œë˜ì—ˆê³  ë¹„ìŠ·í•œ ì˜ë¯¸ì˜ ì§ˆë¬¸ì´ ì¡´ì¬í•œë‹¤ë©´ isNewë¥¼ Falseë¡œ ì„¤ì •í•˜ê³ (ìƒˆë¡­ê²Œ ìƒì„±í•˜ì§€ ì•Šê³  ëŒ€í™” ê¸°ë¡ì—ì„œ ê°€ì ¸ì˜¤ê¸° ë•Œë¬¸ì— Newê°€ ì•„ë‹˜) answerë¥¼ ê·¸ ì™„ë£Œëœ ë‹µë³€ìœ¼ë¡œ ì„¤ì •í•˜ì„¸ìš”.
ë§Œì•½ aiì—ê²Œ ë‹µë³€ì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ê±°ë‚˜ ë¹„ìŠ·í•œ ì§ˆë¬¸ì„ í–ˆë˜ ê¸°ë¡ì´ ì—†ë‹¤ë©´, isNewë¥¼ Trueë¡œ ì„¤ì •í•˜ê³  answerë¥¼ ì‘ì„±í•˜ì§€ ë§ˆì„¸ìš”.
ì£¼ì˜ í•´ì•¼ í•  ì ì€ ëŒ€í™” ê¸°ë¡ì˜ ë§ˆì§€ë§‰ì— ìƒˆë¡œìš´ ì§ˆë¬¸ì´ ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ê·¸ ì§ˆë¬¸ì€ aiì—ê²Œ ë‹µë³€ë˜ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì— ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.

ì ëª…ì‹¬í•˜ì„¸ìš”, ê³¼ê±° ëŒ€í™” ê¸°ë¡ì— ë¹„ìŠ·í•œ ì˜ë¯¸ì˜ ì§ˆë¬¸ì´ ì´ë¯¸ ë‹µë³€ ëœ ì •ë³´ê°€ ìˆë‹¤ë©´ isNewëŠ” Falseê°€ ë˜ë©° ê·¸ aiì—ê²Œ ì´ë¯¸ ë‹µë³€ëœ ë‹µë³€ì„ ìˆ˜ì •í•˜ì§€ ë§ê³  ê·¸ëŒ€ë¡œ answerì— ë„£ìœ¼ì„¸ìš”.
ë§Œì•½ ë¹„ìŠ·í•œ ì˜ë¯¸ì˜ ì§ˆë¬¸ì´ ë‹µë³€ë˜ì§€ ì•Šì•˜ë‹¤ë©´ isNewëŠ” Trueê°€ ë˜ë©° answerëŠ” ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

ë¬¸ì¥ êµ¬ì¡°ê°€ ë¹„ìŠ·í•´ë„ ê°€ë¦¬í‚¤ëŠ” ëŒ€ìƒì´ ë‹¤ë¥´ë‹¤ë©´ ë‹¤ë¥¸ ì§ˆë¬¸ì…ë‹ˆë‹¤,
ë°˜ëŒ€ë¡œ ë¬¸ì¥ êµ¬ì¡°ê°€ ë‹¤ë¥´ë”ë¼ë„ ê°€ë¦¬í‚¤ëŠ” ëŒ€ìƒì´ ê°™ê³  ì˜ë¯¸ê°€ ê°™ì€ ê²½ìš°ê°€ ìˆìœ¼ë‹ˆ ëª¨ë“  ê²ƒì„ ê³ ë ¤í•´ì£¼ì„¸ìš”.

ì§§ì€ ì˜ˆì‹œ: 4B ì—°í•„ì€ ì§„í•œê°€ìš”?ì™€ 2B ì—°í•„ì€ ì§„í•œê°€ìš”? ì€ ë‹¤ë¥¸ ì§ˆë¬¸ì´ë‹ˆ isNewëŠ” Trueì´ë‹¤, ê³ ë¼ë‹ˆì˜ ë¿”ì€ ë¬´ìŠ¨ ìƒ‰ì¸ê°€ìš”?ì™€ ê³ ë¼ë‹ˆê°€ ê°€ì§€ê³  ìˆëŠ” ë¿”ì˜ ìƒ‰ê¹”ì„ ì•Œë ¤ì£¼ì„¸ìš”. ëŠ” ê°™ì€ ì§ˆë¬¸ì´ë‹ˆ isNewê°€ Falseì´ë‹¤.

--------ì˜ˆì‹œ--------
1ë²ˆ ì˜ˆì‹œ:

ì˜ˆì‹œ ê³¼ê±° ëŒ€í™” ê¸°ë¡:
human: í•˜ëŠ˜ì€ ë¬´ìŠ¨ ìƒ‰ì¸ê°€ìš”?
ai: í•˜ëŠ˜ìƒ‰ì…ë‹ˆë‹¤. (ì¶œì²˜: https://example.com/source)
human: ë¬¼í‹°ìŠˆëŠ” ë¬´ì—‡ìœ¼ë¡œ ë§Œë“¤ì–´ì§€ë‚˜ìš”?

ì˜ˆì‹œ ì§ˆë¬¸:
ë¬¼í‹°ìŠˆëŠ” ë¬´ì—‡ìœ¼ë¡œ ë§Œë“¤ì–´ì§€ë‚˜ìš”?

ì˜ˆì‹œ ê²°ê³¼:
isNew = True

||||||||||||||

2ë²ˆ ì˜ˆì‹œ:

ì˜ˆì‹œ ê³¼ê±° ëŒ€í™” ê¸°ë¡:
human: ì—°í•„ì€ ë¬´ì—‡ì„ í•˜ëŠ”ë° ì‚¬ìš©ë˜ë‚˜ìš”?

ì˜ˆì‹œ ì§ˆë¬¸:
ì—°í•„ì€ ë¬´ì—‡ì„ í•˜ëŠ”ë° ì‚¬ìš©ë˜ë‚˜ìš”?

ì˜ˆì‹œ ê²°ê³¼:
isNew = True
answer =

||||||||||||||

3ë²ˆ ì˜ˆì‹œ:

ì˜ˆì‹œ ê³¼ê±° ëŒ€í™” ê¸°ë¡:
human: ë¹›ì˜ ì‚¼ì›ìƒ‰ì€ ë¬´ì—‡ì¸ê°€ìš”?
ai: ë¹¨ê°„ìƒ‰, ì´ˆë¡ìƒ‰, íŒŒë€ìƒ‰ì…ë‹ˆë‹¤. (ì¶œì²˜: https://example.com/source)
human: ì•ˆë…•í•˜ì„¸ìš” ì €ëŠ” Goraniì¸ë°ìš”, ë¹›ì˜ ì‚¼ì›ìƒ‰ì„ ì•Œë ¤ì£¼ì„¸ìš”.

ì˜ˆì‹œ ì§ˆë¬¸:
ì•ˆë…•í•˜ì„¸ìš” ì €ëŠ” Goraniì¸ë°ìš”, ë¹›ì˜ ì‚¼ì›ìƒ‰ì„ ì•Œë ¤ì£¼ì„¸ìš”.

ì˜ˆì‹œ ê²°ê³¼:
isNew = False
answer = ë¹¨ê°„ìƒ‰, ì´ˆë¡ìƒ‰, íŒŒë€ìƒ‰ì…ë‹ˆë‹¤. (ì¶œì²˜: https://example.com/source)

||||||||||||||

3ë²ˆ ì˜ˆì‹œ:

ì˜ˆì‹œ ê³¼ê±° ëŒ€í™” ê¸°ë¡:
human: ë°”ë‹¤ëŠ” ë¬´ì—‡ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆëŠ”ì§€ ê¶ê¸ˆí•´ìš”.
ai: ë°”ë‹¤ëŠ” ëŒ€ë¶€ë¶„ ë¬¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤. (ì¶œì²˜: https://example.com/source)
human: ì•ˆë…•í•˜ì„¸ìš” ë°˜ê°‘ìŠµë‹ˆë‹¤! ë°”ë‹¤ëŠ” ë¬´ì—‡ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆë‚˜ìš”?

ì˜ˆì‹œ ì§ˆë¬¸:
ì•ˆë…•í•˜ì„¸ìš” ë°˜ê°‘ìŠµë‹ˆë‹¤! ë°”ë‹¤ëŠ” ë¬´ì—‡ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆë‚˜ìš”?

ì˜ˆì‹œ ê²°ê³¼:
isNew = False
answer = ë°”ë‹¤ëŠ” ëŒ€ë¶€ë¶„ ë¬¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤. (ì¶œì²˜: https://example.com/source)

||||||||||||||

4ë²ˆ ì˜ˆì‹œ:

ì˜ˆì‹œ ê³¼ê±° ëŒ€í™” ê¸°ë¡:
human: ì¼ê¸°ë¥¼ ì“¸ ë•Œ ì–´ë–¤ ê±¸ ì“°ë©´ ì¢‹ì„ê¹Œ?
ai: ì˜¤ëŠ˜ì˜ ë‚ ì”¨ì™€ ìˆì—ˆë˜ ì¼, ëŠë‚€ ì ì„ ì“°ë©´ ì¢‹ìŠµë‹ˆë‹¤. (ì¶œì²˜: https://example.com/source)
human: ì¼ê¸°ì¥ì„ ì‚´ ë•Œ ì–´ë–¤ ê±¸ ì‚¬ë©´ ì¢‹ì„ê¹Œ?

ì˜ˆì‹œ ì§ˆë¬¸:
ì¼ê¸°ì¥ì„ ì‚´ ë•Œ ì–´ë–¤ ê±¸ ì‚¬ë©´ ì¢‹ì„ê¹Œ?

ì˜ˆì‹œ ê²°ê³¼:
isNew = True
answer =

||||||||||||||

5ë²ˆ ì˜ˆì‹œ:

ì˜ˆì‹œ ê³¼ê±° ëŒ€í™” ê¸°ë¡:
human: ê³ ë¼ë‹ˆëŠ” ë¿”ì´ ëª‡ê°œ ë‹¬ë ¤ ìˆì–´?
ai: ì•”ì»·ì€ ì•ˆ ë‹¬ë ¤ ìˆê³  ìˆ˜ì»·ì€ 2ê°œ ë‹¬ë ¤ ìˆìŠµë‹ˆë‹¤.. (ì¶œì²˜: https://example.com/source)
human: ìœ ë‹ˆì½˜ì€ ë¿”ì´ ëª‡ê°œ ë‹¬ë ¤ ìˆì–´?

ì˜ˆì‹œ ì§ˆë¬¸:
ìœ ë‹ˆì½˜ì€ ë¿”ì´ ëª‡ê°œ ë‹¬ë ¤ ìˆì–´?

ì˜ˆì‹œ ê²°ê³¼:
isNew = True
answer =
--------------------

ì´ì œ ë‹¹ì‹  ì°¨ë¡€ì…ë‹ˆë‹¤!

ì‹¤ì œ ê³¼ê±° ëŒ€í™” ê¸°ë¡:
{history}

ì‹¤ì œ ì§ˆë¬¸:
{question}
"""
)


def paint_history():
    for message in st.session_state["messages"]:
        send_message(message["message"], message["role"], save=False)


def save_message(message, role):
    st.session_state["messages"].append({"message": message, "role": role})


def save_memory(input, output):
    st.session_state["memory"].save_context({"input": input}, {"output": output})


def send_message(message, role, save=True):
    with st.chat_message(role):
        st.markdown(message)
    if save:
        save_message(message, role)


def get_answers(inputs):
    with st.spinner("ë‹µë³€ ìƒì„± ì¤‘.."):
        docs = inputs["docs"]
        question = inputs["question"]
        memory = inputs["memory"]
        answers_chain = answers_prompt | llm
        return {
            "question": question,
            "answers": [
                {
                    "answer": answers_chain.invoke(
                        {
                            "context": doc.page_content,
                            "question": question,
                            "memory": memory,
                        }
                    ).content,
                    "source": doc.metadata["source"],
                    "date": doc.metadata["lastmod"],
                }
                for doc in docs
            ],
        }


def choose_answer(inputs):
    answers = inputs["answers"]
    question = inputs["question"]
    choose_chain = choose_prompt | streaming_llm
    condensed = "\n\n".join(
        f"{answer['answer']}\nì¶œì²˜: {answer['source']}\në‚ ì§œ: {answer['date']}"
        for answer in answers
    )
    return choose_chain.invoke(
        {
            "question": question,
            "answers": condensed,
        }
    )


def invoke_chain(query):
    with st.spinner("ìºì‹œ í™•ì¸ ì¤‘.."):
        condensed = "\n".join(
            f"{message['role']}: {message['message']}"
            for message in st.session_state["messages"]
        )
        cache_chain = cache_prompt | function_llm
        cache_result = json.loads(
            cache_chain.invoke(
                {
                    "history": condensed,
                    "question": query,
                }
            ).additional_kwargs["function_call"]["arguments"]
        )
    if cache_result["isNew"]:
        result = chain.invoke(query)
        save_memory(query, result.content)
    else:
        result = cache_result["answer"]
        st.markdown(result)
        save_message(result, "ai")
        save_memory(query, result)


def parse_page(soup: BeautifulSoup):
    header = soup.find("header")
    footer = soup.find("footer")
    if header:
        header.decompose()
    if footer:
        footer.decompose()
    return str(soup.get_text()).replace("\n", " ").replace("\xa0", " ")


@st.cache_data(show_spinner="ì„ë² ë”© ì¤‘..")
def embed_file(url, _docs):
    cache_dir = LocalFileStore(f"./.cache/site_embeddings/{url.replace('/', '')}")
    embeddings = OpenAIEmbeddings()
    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)
    vector_store = FAISS.from_documents(_docs, cached_embeddings)
    retriever = vector_store.as_retriever()
    print(retriever.invoke("í”ŒëŸ¬í„°"))
    return retriever


@st.cache_data(show_spinner="ì›¹ ì‚¬ì´íŠ¸ ì •ë³´ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘..")
def load_website(url):
    splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
        chunk_size=1000,
        chunk_overlap=200,
    )
    loader = SitemapLoader(
        url,
        parsing_function=parse_page,
    )
    loader.requests_per_second = 1
    docs = loader.load_and_split(text_splitter=splitter)
    retriever = embed_file(url, docs)
    return retriever


st.set_page_config(page_title="SiteGPT", page_icon="ğŸ–¥ï¸")

st.title("SiteGPT")

with st.sidebar:
    url = st.text_input(
        "URLì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.",
        placeholder="https://example.com/sitemap.xml",
    )

if not url:
    st.markdown(
        """
                ì›¹ ì‚¬ì´íŠ¸ì˜ ë‚´ìš©ì— ëŒ€í•´ ê¶ê¸ˆí•œ ê²ƒì„ ì§ˆë¬¸í•´ ë³´ì„¸ìš”.

                ì‚¬ì´ë“œ ë°”ì—ì„œ ì›¹ ì‚¬ì´íŠ¸ì˜ URLì„ ì…ë ¥í•´ì„œ ì‹œì‘í•˜ì„¸ìš”.
    """
    )
    st.session_state["messages"] = []
    st.session_state.memory = ConversationSummaryBufferMemory(
        llm=llm, max_token_limit=1000
    )
else:
    if ".xml" not in url:
        with st.sidebar:
            st.error("ì‚¬ì´íŠ¸ë§µ URLì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
    else:
        header = st.empty()
        paint_history()
        retriever = load_website(url)
        header.info("ì›¹ ì‚¬ì´íŠ¸ ì •ë³´ë¥¼ í•™ìŠµí–ˆìŠµë‹ˆë‹¤. ë¬´ì—‡ì´ë“  ì§ˆë¬¸í•´ ë³´ì„¸ìš”.")
        query = st.chat_input(placeholder="ì›¹ ì‚¬ì´íŠ¸ì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!")
        if query:
            send_message(query, "human")
            chain = (
                {
                    "docs": retriever,
                    "question": RunnablePassthrough(),
                    "memory": st.session_state["memory"].load_memory_variables,
                }
                | RunnableLambda(get_answers)
                | RunnableLambda(choose_answer)
            )
            with st.chat_message("ai"):
                invoke_chain(query)
